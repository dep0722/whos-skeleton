"""
GPUåŠ é€Ÿç‰ˆåºåˆ—æ¨™è¨»è¨“ç·´ç¨‹å¼
è‡ªå‹•åµæ¸¬ä¸¦ä½¿ç”¨NVIDIA GPUé€²è¡Œè¨“ç·´
"""

import pandas as pd
import numpy as np
import os
import glob
from pathlib import Path
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

# =========================
# GPUè¨­å®š - å„ªå…ˆä½¿ç”¨GPU
# =========================
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers, models
    
    # æª¢æ¸¬GPU
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        print("="*60)
        print("ğŸš€ GPUè³‡è¨Š")
        print("="*60)
        for i, gpu in enumerate(gpus):
            print(f"GPU {i}: {gpu}")
        
        # è¨­å®šGPUè¨˜æ†¶é«”å‹•æ…‹å¢é•·(é¿å…ä½”æ»¿æ‰€æœ‰è¨˜æ†¶é«”)
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print("âœ“ GPUè¨˜æ†¶é«”è¨­å®šç‚ºå‹•æ…‹å¢é•·æ¨¡å¼")
        except RuntimeError as e:
            print(f"âš ï¸  GPUè¨­å®šéŒ¯èª¤: {e}")
        
        # é¡¯ç¤ºTensorFlowæœƒä½¿ç”¨å“ªå€‹è£ç½®
        print(f"âœ“ TensorFlowç‰ˆæœ¬: {tf.__version__}")
        print(f"âœ“ å¯ç”¨GPUæ•¸é‡: {len(gpus)}")
        
        # æ¸¬è©¦GPU
        with tf.device('/GPU:0'):
            test_tensor = tf.constant([[1.0, 2.0], [3.0, 4.0]])
            result = tf.matmul(test_tensor, test_tensor)
        print("âœ“ GPUæ¸¬è©¦æˆåŠŸ")
        
    else:
        print("âš ï¸  æœªåµæ¸¬åˆ°GPU,å°‡ä½¿ç”¨CPUè¨“ç·´")
        print("    è«‹ç¢ºèª:")
        print("    1. NVIDIAé©…å‹•ç¨‹å¼å·²å®‰è£")
        print("    2. CUDA Toolkitå·²å®‰è£")
        print("    3. cuDNNå·²å®‰è£")
        print("    4. å®‰è£GPUç‰ˆTensorFlow: pip install tensorflow-gpu")
    
    HAS_TF = True
    
except ImportError:
    HAS_TF = False
    print("âš ï¸  TensorFlowæœªå®‰è£")
    print("è«‹åŸ·è¡Œ: pip install tensorflow-gpu  (GPUç‰ˆ)")
    print("æˆ–:     pip install tensorflow      (CPUç‰ˆ)")

# =========================
# é…ç½®
# =========================
class Config:
    # ===== ä¿®æ”¹é€™è£¡çš„è·¯å¾‘ =====
    ORIGINAL_CSV = r"C:\mydata\sf\open\output_csv\0128\A\0128_A_3.csv"
    SEGMENTS_DIR = r"C:\mydata\sf\open\output_csv\0128cut\A\A_3"
    OUTPUT_DIR = r"C:\mydata\sf\models\segmentation"
    # ========================
    
    # æ¨¡å‹åƒæ•¸
    HIDDEN_SIZE = 128
    NUM_LAYERS = 2
    DROPOUT = 0.3
    LEARNING_RATE = 0.001
    NUM_EPOCHS = 100  # GPUå¯ä»¥è·‘æ›´å¤šepochs
    BATCH_SIZE = 4     # GPUå¯ä»¥ç”¨æ›´å¤§batch size
    
    # ç‰¹å¾µ
    USE_ANKLE_ONLY = True
    USE_VELOCITY = True
    USE_ACCELERATION = True
    
    # æ¨™ç±¤
    LABEL_MAP = {'O': 0, 'B': 1, 'I': 2}
    LABEL_NAMES = ['Outside', 'Begin', 'Inside']
    
    # GPUå°ˆç”¨è¨­å®š
    USE_MIXED_PRECISION = True  # æ··åˆç²¾åº¦è¨“ç·´(æ›´å¿«)
    
config = Config()

# å•Ÿç”¨æ··åˆç²¾åº¦è¨“ç·´(GPUåŠ é€Ÿ)
if HAS_TF and config.USE_MIXED_PRECISION:
    try:
        policy = tf.keras.mixed_precision.Policy('mixed_float16')
        tf.keras.mixed_precision.set_global_policy(policy)
        print("âœ“ å•Ÿç”¨æ··åˆç²¾åº¦è¨“ç·´(FP16)")
    except:
        print("âš ï¸  æ··åˆç²¾åº¦è¨“ç·´ä¸å¯ç”¨")

# =========================
# æ ¸å¿ƒå‡½æ•¸: å¾ç‰‡æ®µè³‡æ–™å¤¾ç”Ÿæˆæ¨™ç±¤
# =========================
def load_segments_and_generate_labels(original_csv_path, segments_dir):
    """
    è®€å–åˆ‡å¥½çš„ç‰‡æ®µCSV,ç”ŸæˆBIOæ¨™ç±¤
    """
    
    print("\n" + "="*60)
    print("è¼‰å…¥è³‡æ–™ä¸¦ç”Ÿæˆæ¨™ç±¤...")
    print("="*60)
    
    # è®€å–åŸå§‹CSV
    if not os.path.exists(original_csv_path):
        print(f"âš ï¸  æª”æ¡ˆä¸å­˜åœ¨: {original_csv_path}")
        print("ä½¿ç”¨æ¸¬è©¦è³‡æ–™...")
        original_csv_path = "/mnt/user-data/uploads/0128_A_3.csv"
    
    df_original = pd.read_csv(original_csv_path)
    n_frames = len(df_original)
    print(f"âœ“ åŸå§‹CSV: {n_frames} å¹€")
    
    # åˆå§‹åŒ–æ¨™ç±¤
    labels = np.zeros(n_frames, dtype=int)
    
    # æª¢æŸ¥ç‰‡æ®µè³‡æ–™å¤¾
    if not os.path.exists(segments_dir):
        print(f"âš ï¸  ç‰‡æ®µè³‡æ–™å¤¾ä¸å­˜åœ¨: {segments_dir}")
        print("ä½¿ç”¨è‡ªå‹•ç”Ÿæˆçš„ç¤ºç¯„æ¨™ç±¤...")
        return df_original, generate_demo_labels(df_original)
    
    # è®€å–æ‰€æœ‰ç‰‡æ®µ
    segment_files = glob.glob(os.path.join(segments_dir, "*.csv"))
    
    if len(segment_files) == 0:
        print(f"âš ï¸  è³‡æ–™å¤¾ä¸­æ²’æœ‰CSVæª”æ¡ˆ: {segments_dir}")
        return df_original, generate_demo_labels(df_original)
    
    print(f"âœ“ æ‰¾åˆ° {len(segment_files)} å€‹ç‰‡æ®µCSV")
    
    segments_info = []
    matched_count = 0
    
    for seg_file in sorted(segment_files):
        try:
            df_seg = pd.read_csv(seg_file)
            seg_name = Path(seg_file).stem
            
            # æ–¹æ³•1: ä½¿ç”¨original_frameæ¬„ä½
            if 'original_frame' in df_seg.columns:
                start_frame = int(df_seg['original_frame'].iloc[0])
                end_frame = int(df_seg['original_frame'].iloc[-1]) + 1
                method = "original_frame"
            
            # æ–¹æ³•2: ç‰¹å¾µåŒ¹é…
            else:
                start_frame, end_frame = match_segment_to_original(df_seg, df_original)
                method = "feature_match"
            
            # é©—è­‰ç¯„åœ
            if start_frame < 0 or end_frame > n_frames or start_frame >= end_frame:
                print(f"  âš ï¸  è·³é {seg_name}: ç„¡æ•ˆç¯„åœ [{start_frame}, {end_frame}]")
                continue
            
            # æ¨™è¨»
            labels[start_frame] = 1  # B
            labels[start_frame+1:end_frame] = 2  # I
            
            segments_info.append({
                'name': seg_name,
                'start': start_frame,
                'end': end_frame,
                'length': end_frame - start_frame,
                'method': method
            })
            matched_count += 1
            
        except Exception as e:
            print(f"  âš ï¸  è®€å–å¤±æ•— {Path(seg_file).name}: {e}")
    
    print(f"\nâœ“ æˆåŠŸåŒ¹é… {matched_count}/{len(segment_files)} å€‹ç‰‡æ®µ")
    print("\nç‰‡æ®µè³‡è¨Š:")
    for seg in segments_info[:10]:
        print(f"  {seg['name']:25s} | [{seg['start']:4d}, {seg['end']:4d}] "
              f"é•·åº¦={seg['length']:3d} | {seg['method']}")
    
    if len(segments_info) > 10:
        print(f"  ... é‚„æœ‰ {len(segments_info)-10} å€‹ç‰‡æ®µ")
    
    # çµ±è¨ˆæ¨™ç±¤åˆ†å¸ƒ
    unique, counts = np.unique(labels, return_counts=True)
    print(f"\næ¨™ç±¤åˆ†å¸ƒ:")
    for label_id, count in zip(unique, counts):
        label_name = config.LABEL_NAMES[label_id]
        print(f"  {label_name:8s} ({label_id}): {count:5d} å¹€ ({count/n_frames*100:5.1f}%)")
    
    return df_original, labels


def match_segment_to_original(df_seg, df_original):
    """é€šéç‰¹å¾µåŒ¹é…æ‰¾åˆ°ç‰‡æ®µä½ç½®"""
    
    match_features = ['Nose_x', 'Nose_y', 'Neck_x', 'Neck_y', 'MidHip_x', 'MidHip_y']
    available_features = [f for f in match_features if f in df_seg.columns and f in df_original.columns]
    
    if len(available_features) == 0:
        available_features = ['LAnkle_x', 'LAnkle_y', 'RAnkle_x', 'RAnkle_y']
    
    first_row = df_seg[available_features].iloc[0].values
    original_features = df_original[available_features].values
    
    distances = np.sqrt(np.sum((original_features - first_row)**2, axis=1))
    start_frame = np.argmin(distances)
    end_frame = start_frame + len(df_seg)
    
    return start_frame, end_frame


def generate_demo_labels(df):
    """å‚™ç”¨: ä½¿ç”¨è¦å‰‡ç”Ÿæˆç¤ºç¯„æ¨™ç±¤"""
    print("  ä½¿ç”¨è¦å‰‡ç”Ÿæˆç¤ºç¯„æ¨™ç±¤...")
    
    labels = [0] * len(df)
    
    FLAT_WINDOW = 5
    FLAT_SLOPE = 0.1
    MIN_FLAT_LEN = 5
    RISE_TOTAL = 3.0
    
    for side in ['L', 'R']:
        col_x = f"{side}Ankle_x"
        if col_x not in df.columns:
            continue
            
        x = df[col_x].values
        n = len(x)
        
        is_flat = np.zeros(n, dtype=bool)
        for i in range(n - FLAT_WINDOW):
            slope = (x[i + FLAT_WINDOW] - x[i]) / FLAT_WINDOW
            if abs(slope) < FLAT_SLOPE:
                is_flat[i:i + FLAT_WINDOW] = True
        
        flat_segments = []
        start = None
        for i, v in enumerate(is_flat):
            if v and start is None:
                start = i
            elif not v and start is not None:
                if i - start >= MIN_FLAT_LEN:
                    flat_segments.append((start, i))
                start = None
        
        for i in range(len(flat_segments) - 1):
            fs, fe = flat_segments[i]
            ns, _ = flat_segments[i + 1]
            
            if abs(x[ns] - x[fe]) > RISE_TOTAL:
                if fs < len(labels) and ns <= len(labels):
                    labels[fs] = 1
                    for j in range(fs + 1, ns):
                        labels[j] = 2
    
    return labels


# =========================
# ç‰¹å¾µæå–
# =========================
def extract_features(df):
    """æå–12ç¶­ç‰¹å¾µå‘é‡"""
    
    features_list = []
    
    ankle_cols = ['LAnkle_x', 'LAnkle_y', 'RAnkle_x', 'RAnkle_y']
    ankle_data = df[ankle_cols].values
    features_list.append(ankle_data)
    
    velocity = np.zeros_like(ankle_data)
    velocity[1:] = ankle_data[1:] - ankle_data[:-1]
    velocity[0] = velocity[1]
    features_list.append(velocity)
    
    acceleration = np.zeros_like(velocity)
    acceleration[1:] = velocity[1:] - velocity[:-1]
    acceleration[0] = acceleration[1]
    features_list.append(acceleration)
    
    features = np.concatenate(features_list, axis=1)
    
    return features


# =========================
# å»ºç«‹GPUå„ªåŒ–çš„æ¨¡å‹
# =========================
def build_lstm_model(input_shape, num_classes):
    """å»ºç«‹GPUå„ªåŒ–çš„é›™å‘LSTMæ¨¡å‹"""
    
    if not HAS_TF:
        return None
    
    # ä½¿ç”¨CuDNNå„ªåŒ–çš„LSTM(GPUå°ˆç”¨,æ›´å¿«)
    model = models.Sequential([
        layers.Bidirectional(
            layers.LSTM(
                config.HIDDEN_SIZE, 
                return_sequences=True,
                # GPUå„ªåŒ–åƒæ•¸
                recurrent_activation='sigmoid',  # CuDNNéœ€è¦
            ),
            input_shape=input_shape
        ),
        layers.Dropout(config.DROPOUT),
        
        layers.Bidirectional(
            layers.LSTM(
                config.HIDDEN_SIZE // 2, 
                return_sequences=True,
                recurrent_activation='sigmoid',
            )
        ),
        layers.Dropout(config.DROPOUT),
        
        layers.TimeDistributed(
            layers.Dense(num_classes, activation='softmax', dtype='float32')
        )
    ])
    
    # ä½¿ç”¨æ··åˆç²¾åº¦æ™‚,è¼¸å‡ºå±¤è¦ç”¨float32
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=config.LEARNING_RATE),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model


# =========================
# GPUè¨“ç·´å›èª¿å‡½æ•¸
# =========================
def create_callbacks(output_dir):
    """å‰µå»ºè¨“ç·´å›èª¿å‡½æ•¸"""
    
    callbacks = []
    
    # 1. ModelCheckpoint - å„²å­˜æœ€ä½³æ¨¡å‹
    checkpoint_path = os.path.join(output_dir, 'best_model.h5')
    checkpoint = keras.callbacks.ModelCheckpoint(
        checkpoint_path,
        monitor='loss',
        save_best_only=True,
        mode='min',
        verbose=1
    )
    callbacks.append(checkpoint)
    
    # 2. EarlyStopping - æå‰åœæ­¢(é˜²æ­¢éæ“¬åˆ)
    early_stop = keras.callbacks.EarlyStopping(
        monitor='loss',
        patience=10,
        restore_best_weights=True,
        verbose=1
    )
    callbacks.append(early_stop)
    
    # 3. ReduceLROnPlateau - å‹•æ…‹èª¿æ•´å­¸ç¿’ç‡
    reduce_lr = keras.callbacks.ReduceLROnPlateau(
        monitor='loss',
        factor=0.5,
        patience=5,
        min_lr=1e-6,
        verbose=1
    )
    callbacks.append(reduce_lr)
    
    # 4. TensorBoard - è¦–è¦ºåŒ–è¨“ç·´éç¨‹
    tensorboard_dir = os.path.join(output_dir, 'tensorboard_logs')
    tensorboard = keras.callbacks.TensorBoard(
        log_dir=tensorboard_dir,
        histogram_freq=1,
        write_graph=True
    )
    callbacks.append(tensorboard)
    
    return callbacks


# =========================
# è¦–è¦ºåŒ–
# =========================
def visualize_results(df, features, true_labels, pred_labels, history=None):
    """ç”Ÿæˆå®Œæ•´è¦–è¦ºåŒ–"""
    
    max_frames = min(500, len(true_labels))
    
    if history is not None:
        fig = plt.figure(figsize=(16, 14))
        gs = fig.add_gridspec(5, 2, hspace=0.3, wspace=0.3)
    else:
        fig = plt.figure(figsize=(16, 10))
        gs = fig.add_gridspec(3, 1, hspace=0.3)
    
    # 1. è»Œè·¡
    ax1 = fig.add_subplot(gs[0, :] if history else gs[0])
    ax1.plot(features[:max_frames, 1], 'b-', label='Left Ankle Y', alpha=0.7, lw=1.5)
    ax1.plot(features[:max_frames, 3], 'r-', label='Right Ankle Y', alpha=0.7, lw=1.5)
    ax1.set_ylabel('Y Position', fontsize=12)
    ax1.set_title('Ankle Trajectories', fontsize=14, fontweight='bold')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. çœŸå¯¦æ¨™ç±¤
    ax2 = fig.add_subplot(gs[1, :] if history else gs[1])
    colors = ['gray', 'green', 'blue']
    for i, label_name in enumerate(config.LABEL_NAMES):
        mask = true_labels[:max_frames] == i
        if np.any(mask):
            frames = np.where(mask)[0]
            ax2.scatter(frames, [1]*len(frames), c=colors[i], 
                       label=label_name, s=20, alpha=0.7)
    ax2.set_ylabel('True Labels', fontsize=12)
    ax2.set_title('Ground Truth Annotations', fontsize=14, fontweight='bold')
    ax2.set_yticks([])
    ax2.legend(loc='upper right')
    ax2.grid(True, alpha=0.3, axis='x')
    ax2.set_xlim([0, max_frames])
    
    # 3. é æ¸¬æ¨™ç±¤
    ax3 = fig.add_subplot(gs[2, :] if history else gs[2])
    for i, label_name in enumerate(config.LABEL_NAMES):
        mask = pred_labels[:max_frames] == i
        if np.any(mask):
            frames = np.where(mask)[0]
            ax3.scatter(frames, [1]*len(frames), c=colors[i], 
                       label=label_name, s=20, alpha=0.7)
    ax3.set_xlabel('Frame Index', fontsize=12)
    ax3.set_ylabel('Predictions', fontsize=12)
    ax3.set_title('Model Predictions', fontsize=14, fontweight='bold')
    ax3.set_yticks([])
    ax3.legend(loc='upper right')
    ax3.grid(True, alpha=0.3, axis='x')
    ax3.set_xlim([0, max_frames])
    
    if history:
        # è¨“ç·´æ›²ç·š
        ax4 = fig.add_subplot(gs[3, 0])
        ax4.plot(history.history['loss'], 'b-', lw=2)
        ax4.set_xlabel('Epoch')
        ax4.set_ylabel('Loss')
        ax4.set_title('Training Loss', fontweight='bold')
        ax4.grid(True, alpha=0.3)
        
        ax5 = fig.add_subplot(gs[3, 1])
        ax5.plot(history.history['accuracy'], 'g-', lw=2)
        ax5.set_xlabel('Epoch')
        ax5.set_ylabel('Accuracy')
        ax5.set_title('Training Accuracy', fontweight='bold')
        ax5.set_ylim([0, 1])
        ax5.grid(True, alpha=0.3)
        
        # æ··æ·†çŸ©é™£
        ax6 = fig.add_subplot(gs[4, :])
        cm = confusion_matrix(true_labels, pred_labels)
        im = ax6.imshow(cm, cmap='Blues')
        ax6.set_xticks(range(3))
        ax6.set_yticks(range(3))
        ax6.set_xticklabels(config.LABEL_NAMES)
        ax6.set_yticklabels(config.LABEL_NAMES)
        ax6.set_xlabel('Predicted')
        ax6.set_ylabel('True')
        ax6.set_title('Confusion Matrix', fontweight='bold')
        
        for i in range(3):
            for j in range(3):
                text = ax6.text(j, i, f'{cm[i, j]}',
                              ha="center", va="center",
                              color="white" if cm[i, j] > cm.max()/2 else "black")
        plt.colorbar(im, ax=ax6)
    
    return fig


# =========================
# ä¸»ç¨‹å¼
# =========================
def main():
    print("\n" + "="*60)
    print("ğŸš€ GPUåŠ é€Ÿåºåˆ—æ¨™è¨»æ¨¡å‹è¨“ç·´")
    print("="*60)
    
    # 1. è¼‰å…¥è³‡æ–™ä¸¦ç”Ÿæˆæ¨™ç±¤
    df, labels = load_segments_and_generate_labels(
        config.ORIGINAL_CSV,
        config.SEGMENTS_DIR
    )
    
    # 2. æå–ç‰¹å¾µ
    print("\næå–ç‰¹å¾µ...")
    features = extract_features(df)
    print(f"âœ“ ç‰¹å¾µå½¢ç‹€: {features.shape}")
    
    # 3. æ¨™æº–åŒ–
    scaler = StandardScaler()
    features = scaler.fit_transform(features)
    
    # 4. æº–å‚™è¨“ç·´è³‡æ–™
    X = features.reshape(1, -1, features.shape[1])
    y = np.array(labels).reshape(1, -1)
    
    # 5. å»ºç«‹ä¸¦è¨“ç·´æ¨¡å‹
    history = None
    
    if HAS_TF:
        print("\n" + "="*60)
        print("å»ºç«‹LSTMæ¨¡å‹...")
        print("="*60)
        
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        os.makedirs(config.OUTPUT_DIR, exist_ok=True)
        
        # å»ºç«‹æ¨¡å‹
        model = build_lstm_model(
            input_shape=(X.shape[1], X.shape[2]),
            num_classes=3
        )
        model.summary()
        
        # å‰µå»ºå›èª¿å‡½æ•¸
        callbacks = create_callbacks(config.OUTPUT_DIR)
        
        print("\n" + "="*60)
        print("é–‹å§‹è¨“ç·´ (ä½¿ç”¨GPUåŠ é€Ÿ)...")
        print("="*60)
        print(f"Epochs: {config.NUM_EPOCHS}")
        print(f"Batch Size: {config.BATCH_SIZE}")
        print(f"Learning Rate: {config.LEARNING_RATE}")
        
        # è¨“ç·´
        import time
        start_time = time.time()
        
        history = model.fit(
            X, y, 
            epochs=config.NUM_EPOCHS,
            batch_size=config.BATCH_SIZE,
            callbacks=callbacks,
            verbose=1
        )
        
        training_time = time.time() - start_time
        print(f"\nâœ“ è¨“ç·´å®Œæˆ! è€—æ™‚: {training_time:.2f} ç§’")
        
        # é æ¸¬
        print("\né€²è¡Œé æ¸¬...")
        predictions = model.predict(X)
        pred_labels = np.argmax(predictions[0], axis=-1)
        
    else:
        print("\nâš ï¸  TensorFlowæœªå®‰è£,è·³éè¨“ç·´")
        pred_labels = labels
    
    # 6. è©•ä¼°
    print("\n" + "="*60)
    print("è©•ä¼°çµæœ")
    print("="*60)
    accuracy = np.mean(pred_labels == labels)
    print(f"æº–ç¢ºç‡: {accuracy:.4f}")
    print("\nåˆ†é¡å ±å‘Š:")
    print(classification_report(labels, pred_labels, target_names=config.LABEL_NAMES))
    
    # 7. è¦–è¦ºåŒ–
    print("\nç”Ÿæˆè¦–è¦ºåŒ–...")
    fig = visualize_results(df, features, labels, pred_labels, history)
    
    # 8. å„²å­˜
    output_dir = "/mnt/user-data/outputs"
    os.makedirs(output_dir, exist_ok=True)
    
    fig_path = os.path.join(output_dir, 'gpu_training_results.png')
    plt.savefig(fig_path, dpi=150, bbox_inches='tight')
    print(f"âœ“ çµæœå·²å„²å­˜: {fig_path}")
    
    if HAS_TF and model:
        model_path = os.path.join(output_dir, 'gpu_segmentation_model.h5')
        model.save(model_path)
        print(f"âœ“ æ¨¡å‹å·²å„²å­˜: {model_path}")
        
        print(f"\nğŸ“Š TensorBoardè¦–è¦ºåŒ–:")
        print(f"   åŸ·è¡Œ: tensorboard --logdir {os.path.join(config.OUTPUT_DIR, 'tensorboard_logs')}")
    
    print("\n" + "="*60)
    print("å®Œæˆ!")
    print("="*60)

if __name__ == "__main__":
    main()
