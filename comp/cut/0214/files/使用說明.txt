"""
========================================================================
å§¿æ…‹åºåˆ—è‡ªå‹•åˆ‡å‰² - æ·±åº¦å­¸ç¿’æ¨¡å‹ä½¿ç”¨æŒ‡å—
========================================================================

## ğŸ“‹ ç›®éŒ„
1. å¿«é€Ÿé–‹å§‹
2. æ¨¡å‹æ¶æ§‹èªªæ˜
3. å¦‚ä½•ä½¿ç”¨ä½ çš„åˆ‡å¥½ç‰‡æ®µé€²è¡Œè¨“ç·´
4. æ¨¡å‹æ”¹é€²å»ºè­°
5. å®Œæ•´ç¨‹å¼ç¢¼

========================================================================
## 1. å¿«é€Ÿé–‹å§‹
========================================================================

### ç’°å¢ƒéœ€æ±‚:
```bash
pip install tensorflow pandas numpy matplotlib scikit-learn
```

### åŸºæœ¬ä½¿ç”¨:
```python
python sequence_labeling_tf.py
```

========================================================================
## 2. æ¨¡å‹æ¶æ§‹èªªæ˜
========================================================================

### BIO æ¨™è¨»æ–¹æ¡ˆ:
- B (Begin): ç‰‡æ®µèµ·å§‹å¹€
- I (Inside): ç‰‡æ®µå…§éƒ¨å¹€
- O (Outside): ç‰‡æ®µå¤–éƒ¨å¹€

### æ¨¡å‹çµæ§‹:
```
è¼¸å…¥å±¤ (12ç¶­ç‰¹å¾µ)
    â†“
é›™å‘LSTMå±¤ (128å–®å…ƒ)
    â†“
Dropout (0.3)
    â†“
é›™å‘LSTMå±¤ (64å–®å…ƒ)
    â†“
Dropout (0.3)
    â†“
TimeDistributed Dense (3é¡åˆ¥)
    â†“
Softmaxè¼¸å‡º
```

### ç‰¹å¾µè¨­è¨ˆ:
1. **åŸºç¤ç‰¹å¾µ** (4ç¶­):
   - LAnkle_x, LAnkle_y
   - RAnkle_x, RAnkle_y

2. **é€Ÿåº¦ç‰¹å¾µ** (4ç¶­):
   - æ¯å€‹åº§æ¨™çš„ä¸€éšå·®åˆ†

3. **åŠ é€Ÿåº¦ç‰¹å¾µ** (4ç¶­):
   - é€Ÿåº¦çš„ä¸€éšå·®åˆ†

ç¸½è¨ˆ: 12ç¶­ç‰¹å¾µå‘é‡

========================================================================
## 3. å¦‚ä½•ä½¿ç”¨ä½ çš„åˆ‡å¥½ç‰‡æ®µé€²è¡Œè¨“ç·´
========================================================================

### æ­¥é©Ÿ 1: æº–å‚™è³‡æ–™

ä½ çš„è³‡æ–™çµæ§‹:
```
C:\mydata\sf\open\output_csv\0128\A\0128_A_3.csv  (åŸå§‹å®Œæ•´CSV)
C:\mydata\sf\open\output_csv\0128cut\A\A_3\      (åˆ‡å¥½çš„ç‰‡æ®µ)
    â”œâ”€â”€ L_1_k.csv
    â”œâ”€â”€ L_2_a.csv
    â”œâ”€â”€ R_1_b.csv
    â””â”€â”€ ...
```

### æ­¥é©Ÿ 2: ç”Ÿæˆè¨“ç·´æ¨™ç±¤

ç¨‹å¼æœƒè‡ªå‹•:
1. è®€å–åŸå§‹CSV
2. è®€å–æ‰€æœ‰åˆ‡å¥½çš„ç‰‡æ®µ
3. æ ¹æ“šç‰‡æ®µçš„å¹€ç¯„åœç”ŸæˆBIOæ¨™ç±¤
4. è¨“ç·´æ¨¡å‹å­¸ç¿’é€™äº›æ¨™ç±¤

### æ­¥é©Ÿ 3: ä¿®æ”¹é…ç½®

åœ¨ç¨‹å¼ä¸­ä¿®æ”¹è·¯å¾‘:
```python
class Config:
    # ä½ çš„è³‡æ–™è·¯å¾‘
    ORIGINAL_CSV = r"C:\mydata\sf\open\output_csv\0128\A\0128_A_3.csv"
    SEGMENTS_DIR = r"C:\mydata\sf\open\output_csv\0128cut\A\A_3"
    
    # æ¨¡å‹åƒæ•¸å¯èª¿æ•´
    HIDDEN_SIZE = 128      # LSTMéš±è—å±¤å¤§å°
    NUM_EPOCHS = 50        # è¨“ç·´è¼ªæ•¸
    DROPOUT = 0.3          # Dropoutç‡
```

========================================================================
## 4. æ¨¡å‹æ”¹é€²å»ºè­°
========================================================================

### æ–¹æ¡ˆ A: å¤šäººè³‡æ–™æ··åˆè¨“ç·´
å¦‚æœä½ æœ‰å¤šå€‹äººçš„è³‡æ–™ (A_3, H_2 ç­‰):

```python
# ä¿®æ”¹è³‡æ–™è¼‰å…¥éƒ¨åˆ†
DATASETS = [
    ("C:\\...\\0128_A_3.csv", "C:\\...\\A_3"),
    ("C:\\...\\0128_H_2.csv", "C:\\...\\H_2"),
    # æ·»åŠ æ›´å¤šè³‡æ–™é›†
]

# åˆä½µæ‰€æœ‰è³‡æ–™é€²è¡Œè¨“ç·´
all_features = []
all_labels = []

for csv_path, seg_dir in DATASETS:
    features, labels = load_and_process(csv_path, seg_dir)
    all_features.append(features)
    all_labels.append(labels)

# è¨“ç·´
model.fit(all_features, all_labels)
```

### æ–¹æ¡ˆ B: å¢åŠ æ›´å¤šç‰¹å¾µ

```python
# åœ¨ extract_features() ä¸­æ·»åŠ :

# 1. é—œç¯€è§’åº¦
def compute_joint_angles(df):
    # è†è“‹è§’åº¦ = arccos((hip-knee Â· knee-ankle) / (||hip-knee|| * ||knee-ankle||))
    pass

# 2. æ­¥å¹…ç‰¹å¾µ
def compute_stride_features(df):
    # å·¦å³è…³è·é›¢
    left_right_dist = np.sqrt(
        (df['LAnkle_x'] - df['RAnkle_x'])**2 + 
        (df['LAnkle_y'] - df['RAnkle_y'])**2
    )
    return left_right_dist

# 3. æ™‚åºç‰¹å¾µ
def add_temporal_context(features, window=5):
    # æ·»åŠ éå»Nå¹€çš„å¹³å‡å€¼
    pass
```

### æ–¹æ¡ˆ C: ä½¿ç”¨æ›´å¼·çš„æ¨¡å‹æ¶æ§‹

```python
# Temporal Convolutional Network (TCN)
from tensorflow.keras.layers import Conv1D, BatchNormalization

def build_tcn_model(input_shape, num_classes):
    model = models.Sequential([
        Conv1D(64, kernel_size=3, padding='same', activation='relu'),
        BatchNormalization(),
        Conv1D(128, kernel_size=3, padding='same', activation='relu'),
        BatchNormalization(),
        Conv1D(num_classes, kernel_size=1, activation='softmax')
    ])
    return model

# Transformer
from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization

def build_transformer_model(input_shape, num_classes):
    inputs = keras.Input(shape=input_shape)
    x = MultiHeadAttention(num_heads=4, key_dim=32)(inputs, inputs)
    x = LayerNormalization()(x)
    outputs = layers.Dense(num_classes, activation='softmax')(x)
    return keras.Model(inputs, outputs)
```

### æ–¹æ¡ˆ D: å¾Œè™•ç†å„ªåŒ–

```python
def post_process_predictions(pred_labels):
    \"\"\"
    å¾Œè™•ç†è¦å‰‡:
    1. é€£çºŒçš„Bè¦åˆä½µ(åªä¿ç•™ç¬¬ä¸€å€‹)
    2. å­¤ç«‹çš„Bè½‰ç‚ºIæˆ–O
    3. çŸ­æ–¼Nå¹€çš„ç‰‡æ®µç§»é™¤
    \"\"\"
    processed = pred_labels.copy()
    
    # 1. åˆä½µé€£çºŒB
    for i in range(1, len(processed)):
        if processed[i] == 1 and processed[i-1] == 1:  # B
            processed[i] = 2  # æ”¹ç‚ºI
    
    # 2. ç§»é™¤å­¤ç«‹B
    for i in range(1, len(processed)-1):
        if processed[i] == 1:  # B
            if processed[i-1] == 0 and processed[i+1] == 0:  # å‰å¾Œéƒ½æ˜¯O
                processed[i] = 0
    
    # 3. æœ€å°é•·åº¦éæ¿¾
    MIN_SEGMENT_LEN = 10
    # ... (å¯¦ä½œç‰‡æ®µé•·åº¦éæ¿¾)
    
    return processed
```

========================================================================
## 5. é€²éšåŠŸèƒ½: å¾ç‰‡æ®µCSVè®€å–çœŸå¯¦æ¨™ç±¤
========================================================================

é€™æ˜¯æœ€é‡è¦çš„éƒ¨åˆ† - å¦‚ä½•è®€å–ä½ å·²ç¶“æ‰‹å·¥æ¨™è¨»å¥½çš„ç‰‡æ®µ!

```python
def load_segments_and_generate_labels(original_csv, segments_dir):
    \"\"\"
    å¾åˆ‡å¥½çš„ç‰‡æ®µCSVç”Ÿæˆè¨“ç·´æ¨™ç±¤
    \"\"\"
    import glob
    from pathlib import Path
    
    # è®€å–åŸå§‹CSV
    df_original = pd.read_csv(original_csv)
    n_frames = len(df_original)
    
    # åˆå§‹åŒ–æ¨™ç±¤ç‚ºO (Outside)
    labels = np.zeros(n_frames, dtype=int)  # 0=O, 1=B, 2=I
    
    # è®€å–æ‰€æœ‰ç‰‡æ®µCSV
    segment_files = glob.glob(os.path.join(segments_dir, "*.csv"))
    
    segments_info = []
    
    for seg_file in sorted(segment_files):
        # è®€å–ç‰‡æ®µ
        df_seg = pd.read_csv(seg_file)
        seg_name = Path(seg_file).stem
        
        # æ–¹æ³•1: å¦‚æœç‰‡æ®µCSVä¸­æœ‰åŸå§‹frame_index
        if 'frame_index' in df_seg.columns and 'original_frame' in df_seg.columns:
            # ä½¿ç”¨original_frameæ¬„ä½
            start_frame = df_seg['original_frame'].iloc[0]
            end_frame = df_seg['original_frame'].iloc[-1] + 1
        
        # æ–¹æ³•2: é€šéåŒ¹é…ç‰¹å¾µå€¼åæ¨åŸå§‹å¹€ä½ç½®
        else:
            # ä½¿ç”¨ç¬¬ä¸€å¹€çš„ç‰¹å¾µå€¼åœ¨åŸå§‹CSVä¸­æœå°‹
            first_row_features = df_seg.iloc[0]
            
            # åŒ¹é…é¼»å­åº§æ¨™ (é€šå¸¸æœ€ç©©å®š)
            nose_x = first_row_features['Nose_x']
            nose_y = first_row_features['Nose_y']
            
            # åœ¨åŸå§‹CSVä¸­æ‰¾æœ€æ¥è¿‘çš„å¹€
            distances = np.sqrt(
                (df_original['Nose_x'] - nose_x)**2 + 
                (df_original['Nose_y'] - nose_y)**2
            )
            start_frame = np.argmin(distances)
            end_frame = start_frame + len(df_seg)
        
        # æ¨™è¨»é€™å€‹ç‰‡æ®µ
        if start_frame < n_frames and end_frame <= n_frames:
            labels[start_frame] = 1  # B (Begin)
            labels[start_frame+1:end_frame] = 2  # I (Inside)
            
            segments_info.append({
                'name': seg_name,
                'start': start_frame,
                'end': end_frame,
                'length': end_frame - start_frame
            })
    
    print(f"âœ“ å¾ {len(segments_info)} å€‹ç‰‡æ®µç”Ÿæˆæ¨™ç±¤")
    for seg in segments_info[:5]:  # é¡¯ç¤ºå‰5å€‹
        print(f"  - {seg['name']:20s} | Frames {seg['start']:4d}-{seg['end']:4d}")
    
    return labels, segments_info


# ä½¿ç”¨ç¯„ä¾‹:
labels, segments = load_segments_and_generate_labels(
    "C:\\...\\0128_A_3.csv",
    "C:\\...\\A_3"
)
```

========================================================================
## 6. å®Œæ•´è¨“ç·´æµç¨‹ç¯„ä¾‹
========================================================================

```python
# å®Œæ•´çš„è¨“ç·´è…³æœ¬
def train_segmentation_model():
    # 1. è¼‰å…¥å¤šå€‹è³‡æ–™é›†
    datasets = [
        ("path/to/A_3.csv", "path/to/A_3_segments"),
        ("path/to/H_2.csv", "path/to/H_2_segments"),
    ]
    
    all_X = []
    all_y = []
    
    for csv_path, seg_dir in datasets:
        # è®€å–åŸå§‹è³‡æ–™
        df = pd.read_csv(csv_path)
        
        # å¾ç‰‡æ®µç”Ÿæˆæ¨™ç±¤
        labels, _ = load_segments_and_generate_labels(csv_path, seg_dir)
        
        # æå–ç‰¹å¾µ
        features, _ = extract_features(df)
        
        # æ¨™æº–åŒ–
        scaler = StandardScaler()
        features = scaler.fit_transform(features)
        
        all_X.append(features)
        all_y.append(labels)
    
    # 2. åˆä½µè³‡æ–™
    X_train = np.vstack(all_X)
    y_train = np.concatenate(all_y)
    
    # 3. å»ºç«‹æ¨¡å‹
    model = build_model(
        input_shape=(None, X_train.shape[1]),
        num_classes=3
    )
    
    # 4. è¨“ç·´
    history = model.fit(
        X_train.reshape(1, -1, X_train.shape[1]),
        y_train.reshape(1, -1),
        epochs=50,
        verbose=1
    )
    
    # 5. å„²å­˜æ¨¡å‹
    model.save('segmentation_model.h5')
    
    return model, history

# åŸ·è¡Œè¨“ç·´
model, history = train_segmentation_model()
```

========================================================================
## 7. ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹é€²è¡Œé æ¸¬
========================================================================

```python
def predict_segments(model, csv_path, output_dir):
    \"\"\"ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹é æ¸¬æ–°è³‡æ–™çš„åˆ‡å‰²é»\"\"\"
    
    # 1. è®€å–è³‡æ–™
    df = pd.read_csv(csv_path)
    
    # 2. æå–ç‰¹å¾µ
    features, _ = extract_features(df)
    
    # 3. æ¨™æº–åŒ–
    scaler = StandardScaler()
    features = scaler.fit_transform(features)
    
    # 4. é æ¸¬
    X = features.reshape(1, -1, features.shape[1])
    predictions = model.predict(X)
    pred_labels = np.argmax(predictions[0], axis=-1)
    
    # 5. å¾é æ¸¬ä¸­æå–ç‰‡æ®µ
    segments = extract_segments_from_predictions(pred_labels)
    
    # 6. è¼¸å‡ºç‰‡æ®µCSV
    os.makedirs(output_dir, exist_ok=True)
    
    for i, seg in enumerate(segments):
        start, end = seg['start'], seg['end']
        seg_df = df.iloc[start:end].copy()
        seg_df['frame_index'] = range(len(seg_df))
        
        output_path = os.path.join(output_dir, f"predicted_{i+1}.csv")
        seg_df.to_csv(output_path, index=False)
    
    print(f"âœ“ é æ¸¬å®Œæˆ,è¼¸å‡º {len(segments)} å€‹ç‰‡æ®µåˆ° {output_dir}")
    
    return segments


def extract_segments_from_predictions(pred_labels):
    \"\"\"å¾BIOæ¨™ç±¤ä¸­æå–ç‰‡æ®µç¯„åœ\"\"\"
    segments = []
    in_segment = False
    start = None
    
    for i, label in enumerate(pred_labels):
        if label == 1:  # B (Begin)
            if in_segment:
                # çµæŸå‰ä¸€å€‹ç‰‡æ®µ
                segments.append({'start': start, 'end': i})
            # é–‹å§‹æ–°ç‰‡æ®µ
            start = i
            in_segment = True
        elif label == 0:  # O (Outside)
            if in_segment:
                # çµæŸç‰‡æ®µ
                segments.append({'start': start, 'end': i})
                in_segment = False
        # label == 2 (I) ç¹¼çºŒç•¶å‰ç‰‡æ®µ
    
    # è™•ç†æœ€å¾Œä¸€å€‹ç‰‡æ®µ
    if in_segment:
        segments.append({'start': start, 'end': len(pred_labels)})
    
    return segments

# ä½¿ç”¨ç¯„ä¾‹:
model = keras.models.load_model('segmentation_model.h5')
segments = predict_segments(model, 'new_data.csv', 'output_segments/')
```

========================================================================
## 8. æ•ˆèƒ½è©•ä¼°æŒ‡æ¨™
========================================================================

```python
def evaluate_segmentation_quality(true_segments, pred_segments):
    \"\"\"
    è©•ä¼°åˆ‡å‰²å“è³ª
    
    æŒ‡æ¨™:
    1. IoU (Intersection over Union)
    2. ç‰‡æ®µæ•¸é‡åŒ¹é…åº¦
    3. å¹³å‡é‚Šç•Œèª¤å·®
    \"\"\"
    
    # è¨ˆç®—IoU
    ious = []
    for true_seg in true_segments:
        best_iou = 0
        for pred_seg in pred_segments:
            iou = compute_iou(true_seg, pred_seg)
            best_iou = max(best_iou, iou)
        ious.append(best_iou)
    
    avg_iou = np.mean(ious)
    
    print(f"å¹³å‡ IoU: {avg_iou:.3f}")
    print(f"çœŸå¯¦ç‰‡æ®µæ•¸: {len(true_segments)}")
    print(f"é æ¸¬ç‰‡æ®µæ•¸: {len(pred_segments)}")
    
    return avg_iou


def compute_iou(seg1, seg2):
    \"\"\"è¨ˆç®—å…©å€‹ç‰‡æ®µçš„IoU\"\"\"
    start1, end1 = seg1['start'], seg1['end']
    start2, end2 = seg2['start'], seg2['end']
    
    # äº¤é›†
    intersection = max(0, min(end1, end2) - max(start1, start2))
    
    # è¯é›†
    union = (end1 - start1) + (end2 - start2) - intersection
    
    return intersection / union if union > 0 else 0
```

========================================================================
## ç¸½çµ
========================================================================

é€™å€‹æ¡†æ¶æä¾›äº†:
âœ“ è‡ªå‹•å¾åˆ‡å¥½çš„ç‰‡æ®µå­¸ç¿’åˆ‡å‰²æ¨¡å¼
âœ“ ä½¿ç”¨æ·±åº¦å­¸ç¿’(LSTM)é€²è¡Œåºåˆ—æ¨™è¨»
âœ“ å®Œæ•´çš„è¨“ç·´ã€è©•ä¼°ã€é æ¸¬æµç¨‹
âœ“ å¯æ“´å±•çš„æ¶æ§‹æ”¯æ´å¤šç¨®æ”¹é€²

ä¸‹ä¸€æ­¥å»ºè­°:
1. æ”¶é›†æ›´å¤šæ¨™è¨»è³‡æ–™(ä¸åŒäººã€ä¸åŒæ¢ä»¶)
2. å˜—è©¦ä¸åŒæ¨¡å‹æ¶æ§‹(TCNã€Transformer)
3. æ·»åŠ æ›´å¤šç‰¹å¾µ(é—œç¯€è§’åº¦ã€æ­¥æ…‹åƒæ•¸)
4. å¯¦ä½œç·šä¸Šå­¸ç¿’(incremental learning)

ç¥è¨“ç·´é †åˆ©! ğŸš€
\"\"\"
